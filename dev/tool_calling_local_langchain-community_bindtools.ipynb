{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df46938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93a3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    \"\"\"\n",
    "    Clear CUDA memory cache to free up GPU resources between queries.\n",
    "    Only has an effect if CUDA is available.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"CUDA memory cache cleared\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38f390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cache cleared\n"
     ]
    }
   ],
   "source": [
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864d9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"qwen3:1.7b\" #works: qwen3:1.7b, qwen3:latest, hir0rameel/qwen-claude:latest, mistral-small3.2:latest\n",
    "    #not working (consistently): llama3.2, qwq, qwen3:0.6b, qwen3:14b,qwen3:30b-a3b, deepseek-r1:latest, atombuild/deepseek-r1-claude3.7:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514ed4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=\"ollama\",  # dummy key\n",
    "    model=llm_model,\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7accc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(location: str = \"Unknown\", temperature_format: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a specific location. ONLY use this tool when the user explicitly asks about weather conditions, temperature, or weather forecasts for a particular place.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get weather for\n",
    "        temperature_format: Temperature format (celsius or fahrenheit)\n",
    "    \"\"\"\n",
    "    return f\"Sample weather for {location} in {temperature_format} is 20° and sunny!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb928c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0bb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_tools(user_input: str, verbose: bool = True):\n",
    "    \"\"\"Execute a query with tool support using bound tools\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n> Entering new chat session...\")\n",
    "        print(f\"> User Input: {user_input}\")\n",
    "    \n",
    "    # Create messages\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    # Get initial response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"> AI Response: {response.content}\")\n",
    "    \n",
    "    # Check if response contains tool calls\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        if verbose:\n",
    "            print(f\"> Tool calls detected: {len(response.tool_calls)}\")\n",
    "        \n",
    "        # Add AI message to conversation\n",
    "        messages.append(response)\n",
    "        \n",
    "        # Execute each tool call\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call['name']\n",
    "            tool_args = tool_call['args']\n",
    "            tool_id = tool_call['id']\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"> Calling tool: {tool_name} with args: {tool_args}\")\n",
    "            \n",
    "            try:\n",
    "                # FIX: Use tool's invoke method instead of direct call\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    result = get_current_weather.invoke(tool_args)  # Use .invoke() method\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"> Tool result: {result}\")\n",
    "                    \n",
    "                    # Add tool result to messages\n",
    "                    messages.append(ToolMessage(\n",
    "                        content=result,\n",
    "                        tool_call_id=tool_id\n",
    "                    ))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"> Tool execution error: {e}\")\n",
    "                \n",
    "                # Add error message to conversation\n",
    "                messages.append(ToolMessage(\n",
    "                    content=f\"Error executing tool: {str(e)}\",\n",
    "                    tool_call_id=tool_id\n",
    "                ))\n",
    "        \n",
    "        # Get final response incorporating tool results\n",
    "        final_response = llm.invoke(messages)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"> Final Answer: {final_response.content}\")\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"output\": final_response.content,\n",
    "            \"intermediate_steps\": [(tool_call, \"executed\") for tool_call in response.tool_calls]\n",
    "        }\n",
    "    else:\n",
    "        # No tools needed, return direct response\n",
    "        if verbose:\n",
    "            print(f\"> No tools needed, returning direct response\")\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"output\": response.content,\n",
    "            \"intermediate_steps\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5b8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative simpler approach without manual message handling\n",
    "def simple_tool_execution(user_input: str):\n",
    "    \"\"\"Simplified tool execution - let OpenAI interface handle everything\"\"\"\n",
    "    response = llm_with_tools.invoke([HumanMessage(content=user_input)])\n",
    "    \n",
    "    # If tool calls exist, the response should already include tool execution\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        # Process tool calls manually if needed\n",
    "        messages = [HumanMessage(content=user_input), response]\n",
    "        \n",
    "        for tool_call in response.tool_calls:\n",
    "            if tool_call['name'] == \"get_current_weather\":\n",
    "                result = get_current_weather(**tool_call['args'])\n",
    "                messages.append(ToolMessage(\n",
    "                    content=result,\n",
    "                    tool_call_id=tool_call['id']\n",
    "                ))\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = llm.invoke(messages)\n",
    "        return final_response.content\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0087480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weather Test Iteration 0 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: <think>\n",
      "Okay, the user is asking about the weather in Aachen, specifically in Celsius. Let me check the tools available. There's a function called get_current_weather. The parameters required are location and temperature_format. The user mentioned Aachen as the location and wants Celsius, so I need to set location to \"Aachen\" and temperature_format to \"celsius\". I should call that function with these parameters. Let me make sure I didn't miss any required fields. The required section is null, so both parameters are optional. Alright, that's all I need.\n",
      "</think>\n",
      "\n",
      "\n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: <think>\n",
      "Okay, the user asked for the weather in Aachen in Celsius, and the assistant provided a sample answer of 20°C and sunny. Let me check if that's accurate.\n",
      "\n",
      "First, I need to verify the current weather in Aachen. As of my last update (October 2023), Aachen, Germany, has a temperate climate with warm summers and cold winters. The average temperature in summer is around 20°C, and in winter, around 0°C. So 20°C in summer makes sense. \n",
      "\n",
      "The user's response mentions \"sample weather,\" which implies it's not real-time. The assistant's answer is plausible given the typical weather patterns. However, I should note that the actual temperature can vary. Also, the weather condition \"sunny\" is accurate for summer in Aachen. \n",
      "\n",
      "I should confirm if the assistant's answer is correct based on the data. Since the assistant provided a reasonable estimate, it's acceptable. But I should also mention that temperatures can fluctuate and suggest checking a reliable weather source for the most up-to-date info.\n",
      "</think>\n",
      "\n",
      "The sample weather for Aachen in Celsius is **20°C** and **sunny**, which aligns with the typical summer climate in the region. However, temperatures can vary, and it's always best to check the latest forecast for precise conditions. For real-time data, you can use a weather service like Weather.com or AccuWeather.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': '<think>\\nOkay, the user asked for the weather in Aachen in Celsius, and the assistant provided a sample answer of 20°C and sunny. Let me check if that\\'s accurate.\\n\\nFirst, I need to verify the current weather in Aachen. As of my last update (October 2023), Aachen, Germany, has a temperate climate with warm summers and cold winters. The average temperature in summer is around 20°C, and in winter, around 0°C. So 20°C in summer makes sense. \\n\\nThe user\\'s response mentions \"sample weather,\" which implies it\\'s not real-time. The assistant\\'s answer is plausible given the typical weather patterns. However, I should note that the actual temperature can vary. Also, the weather condition \"sunny\" is accurate for summer in Aachen. \\n\\nI should confirm if the assistant\\'s answer is correct based on the data. Since the assistant provided a reasonable estimate, it\\'s acceptable. But I should also mention that temperatures can fluctuate and suggest checking a reliable weather source for the most up-to-date info.\\n</think>\\n\\nThe sample weather for Aachen in Celsius is **20°C** and **sunny**, which aligns with the typical summer climate in the region. However, temperatures can vary, and it\\'s always best to check the latest forecast for precise conditions. For real-time data, you can use a weather service like Weather.com or AccuWeather.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_6gk0vqdo', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 1 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: <think>\n",
      "Okay, the user is asking for the weather in Aachen in Celsius. Let me check the tools available. There's a function called get_current_weather. The parameters required are location and temperature_format. The user specified Aachen as the location and wants Celsius. So I need to call that function with those exact parameters. I should make sure the arguments are in JSON format within the tool_call tags. Let me structure the response correctly.\n",
      "</think>\n",
      "\n",
      "\n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: <think>\n",
      "Okay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\n",
      "</think>\n",
      "\n",
      "The current weather in Aachen is **20°C** and **sunny**.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': '<think>\\nOkay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\\n</think>\\n\\nThe current weather in Aachen is **20°C** and **sunny**.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_2shl2wn5', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 2 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: <think>\n",
      "Okay, the user is asking for the weather in Aachen in Celsius. Let me check the tools available. There's a function called get_current_weather. The parameters required are location and temperature_format. The user specified Aachen as the location and wants Celsius. So I need to call that function with those exact parameters. I should make sure the arguments are in JSON format within the tool_call tags. Let me structure the response correctly.\n",
      "</think>\n",
      "\n",
      "\n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: <think>\n",
      "Okay, the user asked for the weather in Aachen in Celsius, and the assistant provided a sample response of 20°C and sunny. Let me check if that's accurate. Wait, the assistant's response is just a sample, not actual data. The user might be testing if I can recognize that the answer is hypothetical. I should clarify that the actual weather can vary and suggest checking a weather service for real-time data. Also, make sure to keep the response friendly and helpful.\n",
      "</think>\n",
      "\n",
      "The weather in Aachen can vary, but a typical sunny day might have temperatures around **15–20°C**. For real-time updates, check a weather service like Weather.com or AccuWeather. Let me know if you'd like a forecast for a specific date! 🌞\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': \"<think>\\nOkay, the user asked for the weather in Aachen in Celsius, and the assistant provided a sample response of 20°C and sunny. Let me check if that's accurate. Wait, the assistant's response is just a sample, not actual data. The user might be testing if I can recognize that the answer is hypothetical. I should clarify that the actual weather can vary and suggest checking a weather service for real-time data. Also, make sure to keep the response friendly and helpful.\\n</think>\\n\\nThe weather in Aachen can vary, but a typical sunny day might have temperatures around **15–20°C**. For real-time updates, check a weather service like Weather.com or AccuWeather. Let me know if you'd like a forecast for a specific date! 🌞\", 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_z2ziwa2u', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 3 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: <think>\n",
      "Okay, the user is asking for the weather in Aachen in Celsius. Let me check the tools available. There's a function called get_current_weather. The parameters required are location and temperature_format. The user specified Aachen as the location and wants Celsius. So I need to call that function with those exact parameters. I should make sure the arguments are in JSON format within the tool_call tags. Let me structure the response correctly.\n",
      "</think>\n",
      "\n",
      "\n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: <think>\n",
      "Okay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\n",
      "</think>\n",
      "\n",
      "The current weather in Aachen is **20°C** and **sunny**.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': '<think>\\nOkay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\\n</think>\\n\\nThe current weather in Aachen is **20°C** and **sunny**.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_2lbkdsr4', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 4 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: <think>\n",
      "Okay, the user is asking for the weather in Aachen in Celsius. Let me check the tools available. There's a function called get_current_weather. The parameters required are location and temperature_format. The user specified Aachen as the location and wants Celsius. So I need to call that function with those exact parameters. I should make sure the arguments are in JSON format within the tool_call tags. Let me structure the response correctly.\n",
      "</think>\n",
      "\n",
      "\n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: <think>\n",
      "Okay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\n",
      "</think>\n",
      "\n",
      "The current weather in Aachen is **20°C** and **sunny**.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': '<think>\\nOkay, the user asked for the weather in Aachen in Celsius. I need to check the current temperature. Let me use the get_current_weather function. The location is Aachen, and the temperature format should be Celsius. The function returns 20°C. I should present this information clearly. Also, note that the weather is sunny. Make sure the response is concise and includes both the temperature and the condition.\\n</think>\\n\\nThe current weather in Aachen is **20°C** and **sunny**.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_c2iohrq0', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Test weather query\n",
    "for i in range(5):  # Reduced iterations for testing\n",
    "    print(f\"\\n=== Weather Test Iteration {i} ===\")\n",
    "    response = execute_with_tools(\"What's the weather in Aachen, in celsius?\", verbose=True)\n",
    "    print(f\"Final response: {response}\")\n",
    "    clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74aa90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Simple Approach ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_784397/2307705365.py:13: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = get_current_weather(**tool_call['args'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseTool.__call__() got an unexpected keyword argument 'location'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Alternative: Using the simpler approach\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Testing Simple Approach ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m weather_response = \u001b[43msimple_tool_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather in Berlin?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeather response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweather_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msimple_tool_execution\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response.tool_calls:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_call[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mget_current_weather\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         result = \u001b[43mget_current_weather\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m         messages.append(ToolMessage(\n\u001b[32m     15\u001b[39m             content=result,\n\u001b[32m     16\u001b[39m             tool_call_id=tool_call[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m         ))\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Get final response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/dev/langgraph/local-rag-researcher-deepseek-he/lrrd-venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseTool.__call__() got an unexpected keyword argument 'location'"
     ]
    }
   ],
   "source": [
    "# Alternative: Using the simpler approach\n",
    "print(\"\\n=== Testing Simple Approach ===\")\n",
    "weather_response = simple_tool_execution(\"What's the weather in Berlin?\")\n",
    "print(f\"Weather response: {weather_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrrd-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
