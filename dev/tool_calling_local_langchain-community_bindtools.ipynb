{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df46938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93a3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    \"\"\"\n",
    "    Clear CUDA memory cache to free up GPU resources between queries.\n",
    "    Only has an effect if CUDA is available.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"CUDA memory cache cleared\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38f390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cache cleared\n"
     ]
    }
   ],
   "source": [
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864d9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-oss:20b\" #works: qwen3:1.7b, qwen3:latest, hir0rameel/qwen-claude:latest, mistral-small3.2:latest\n",
    "    #not working (consistently): llama3.2, qwq, qwen3:0.6b, qwen3:14b,qwen3:30b-a3b, deepseek-r1:latest, atombuild/deepseek-r1-claude3.7:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514ed4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=\"ollama\",  # dummy key\n",
    "    model=llm_model,\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7accc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(location: str = \"Unknown\", temperature_format: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a specific location. ONLY use this tool when the user explicitly asks about weather conditions, temperature, or weather forecasts for a particular place.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get weather for\n",
    "        temperature_format: Temperature format (celsius or fahrenheit)\n",
    "    \"\"\"\n",
    "    return f\"Sample weather for {location} in {temperature_format} is 20° and sunny!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb928c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0bb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_tools(user_input: str, verbose: bool = True):\n",
    "    \"\"\"Execute a query with tool support using bound tools\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n> Entering new chat session...\")\n",
    "        print(f\"> User Input: {user_input}\")\n",
    "    \n",
    "    # Create messages\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    # Get initial response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"> AI Response: {response.content}\")\n",
    "    \n",
    "    # Check if response contains tool calls\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        if verbose:\n",
    "            print(f\"> Tool calls detected: {len(response.tool_calls)}\")\n",
    "        \n",
    "        # Add AI message to conversation\n",
    "        messages.append(response)\n",
    "        \n",
    "        # Execute each tool call\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call['name']\n",
    "            tool_args = tool_call['args']\n",
    "            tool_id = tool_call['id']\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"> Calling tool: {tool_name} with args: {tool_args}\")\n",
    "            \n",
    "            try:\n",
    "                # FIX: Use tool's invoke method instead of direct call\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    result = get_current_weather.invoke(tool_args)  # Use .invoke() method\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"> Tool result: {result}\")\n",
    "                    \n",
    "                    # Add tool result to messages\n",
    "                    messages.append(ToolMessage(\n",
    "                        content=result,\n",
    "                        tool_call_id=tool_id\n",
    "                    ))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"> Tool execution error: {e}\")\n",
    "                \n",
    "                # Add error message to conversation\n",
    "                messages.append(ToolMessage(\n",
    "                    content=f\"Error executing tool: {str(e)}\",\n",
    "                    tool_call_id=tool_id\n",
    "                ))\n",
    "        \n",
    "        # Get final response incorporating tool results\n",
    "        final_response = llm.invoke(messages)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"> Final Answer: {final_response.content}\")\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"output\": final_response.content,\n",
    "            \"intermediate_steps\": [(tool_call, \"executed\") for tool_call in response.tool_calls]\n",
    "        }\n",
    "    else:\n",
    "        # No tools needed, return direct response\n",
    "        if verbose:\n",
    "            print(f\"> No tools needed, returning direct response\")\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"output\": response.content,\n",
    "            \"intermediate_steps\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5b8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative simpler approach without manual message handling\n",
    "def simple_tool_execution(user_input: str):\n",
    "    \"\"\"Simplified tool execution - let OpenAI interface handle everything\"\"\"\n",
    "    response = llm_with_tools.invoke([HumanMessage(content=user_input)])\n",
    "    \n",
    "    # If tool calls exist, the response should already include tool execution\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        # Process tool calls manually if needed\n",
    "        messages = [HumanMessage(content=user_input), response]\n",
    "        \n",
    "        for tool_call in response.tool_calls:\n",
    "            if tool_call['name'] == \"get_current_weather\":\n",
    "                result = get_current_weather(**tool_call['args'])\n",
    "                messages.append(ToolMessage(\n",
    "                    content=result,\n",
    "                    tool_call_id=tool_call['id']\n",
    "                ))\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = llm.invoke(messages)\n",
    "        return final_response.content\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0087480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weather Test Iteration 0 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: \n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: \n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': '', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_06eij6zy', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 1 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: \n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: The current weather in Aachen is **20 °C** and sunny.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': 'The current weather in Aachen is **20\\u202f°C** and sunny.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_pd37qtcg', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 2 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: \n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: I don’t have live data, but the function call returned a sample forecast for Aachen: **20 °C and sunny**. If you need the up‑to‑date weather, you’ll want to check a reliable weather service or app for the latest conditions.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': 'I don’t have live data, but the function call returned a sample forecast for Aachen: **20\\u202f°C and sunny**. If you need the up‑to‑date weather, you’ll want to check a reliable weather service or app for the latest conditions.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_klgo0e16', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 3 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: \n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: The current weather in Aachen is **20 °C** and sunny.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': 'The current weather in Aachen is **20\\u202f°C** and sunny.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_tr2an3jf', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n",
      "\n",
      "=== Weather Test Iteration 4 ===\n",
      "\n",
      "> Entering new chat session...\n",
      "> User Input: What's the weather in Aachen, in celsius?\n",
      "> AI Response: \n",
      "> Tool calls detected: 1\n",
      "> Calling tool: get_current_weather with args: {'location': 'Aachen', 'temperature_format': 'celsius'}\n",
      "> Tool result: Sample weather for Aachen in celsius is 20° and sunny!\n",
      "> Final Answer: The current weather in Aachen is **20 °C** and sunny.\n",
      "Final response: {'input': \"What's the weather in Aachen, in celsius?\", 'output': 'The current weather in Aachen is **20\\u202f°C** and sunny.', 'intermediate_steps': [({'name': 'get_current_weather', 'args': {'location': 'Aachen', 'temperature_format': 'celsius'}, 'id': 'call_nuukl86q', 'type': 'tool_call'}, 'executed')]}\n",
      "CUDA memory cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Test weather query\n",
    "for i in range(5):  # Reduced iterations for testing\n",
    "    print(f\"\\n=== Weather Test Iteration {i} ===\")\n",
    "    response = execute_with_tools(\"What's the weather in Aachen, in celsius?\", verbose=True)\n",
    "    print(f\"Final response: {response}\")\n",
    "    clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74aa90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Simple Approach ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1418399/2307705365.py:13: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = get_current_weather(**tool_call['args'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseTool.__call__() got an unexpected keyword argument 'location'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Alternative: Using the simpler approach\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Testing Simple Approach ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m weather_response = \u001b[43msimple_tool_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather in Berlin?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeather response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweather_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msimple_tool_execution\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response.tool_calls:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_call[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mget_current_weather\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         result = \u001b[43mget_current_weather\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m         messages.append(ToolMessage(\n\u001b[32m     15\u001b[39m             content=result,\n\u001b[32m     16\u001b[39m             tool_call_id=tool_call[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m         ))\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Get final response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/dev/langgraph/local-rag-researcher-deepseek-he/lrrd-venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseTool.__call__() got an unexpected keyword argument 'location'"
     ]
    }
   ],
   "source": [
    "# Alternative: Using the simpler approach\n",
    "print(\"\\n=== Testing Simple Approach ===\")\n",
    "weather_response = simple_tool_execution(\"What's the weather in Berlin?\")\n",
    "print(f\"Weather response: {weather_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrrd-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
